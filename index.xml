<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>JAEHYUK PARK on JAEHYUK PARK</title>
    <link>https://jaehyukpark.github.io/</link>
    <description>Recent content in JAEHYUK PARK on JAEHYUK PARK</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <copyright>&amp;copy; 2018</copyright>
    <lastBuildDate>Tue, 17 Jul 2018 00:00:00 -0400</lastBuildDate>
    <atom:link href="/" rel="self" type="application/rss+xml" />
    
    <item>
      <title>Dividing train/dev/test sets in machine learning</title>
      <link>https://jaehyukpark.github.io/post/train_dev_test_set/</link>
      <pubDate>Fri, 27 Jul 2018 22:44:28 -0400</pubDate>
      
      <guid>https://jaehyukpark.github.io/post/train_dev_test_set/</guid>
      <description>

&lt;h1 id=&#34;1-why-does-it-matter-or-when-does-it-matter&#34;&gt;1. Why Does It Matter? Or When Does It Matter?&lt;/h1&gt;

&lt;p&gt;In one of my current projects, our team try to generate a machine learning model
to predict whether some rare event would be happened or not.
Since we don&amp;rsquo;t want to use any information from our test set, even in overall
descriptive statistics, how to divide our whole dataset into train/dev/test
sets becomes one of our concern before further analyses.
Training set is the data to be used for training our model,
and dev set is the data to compare and select among multiple models
(having different hyperparameters, layers, evaluation functions, etc.),
while test set is to finally test the accuarcy of our selected model.&lt;/p&gt;

&lt;p&gt;Hence, I decided to survey some tips and advices from existing works
such as machine learning blog articles and academic papers.
Although some of them are not related to our current concern, I
put it here if it seems useful in other future cases.&lt;/p&gt;

&lt;h1 id=&#34;2-advices-from-blogs-and-online-lectures&#34;&gt;2. Advices from blogs and online lectures&lt;/h1&gt;

&lt;h2 id=&#34;1-sequential-splitting-vs-random-splitting&#34;&gt;(1) Sequential splitting vs. Random splitting&lt;/h2&gt;

&lt;p&gt;&lt;a href=&#34;https://docs.aws.amazon.com/machine-learning/latest/dg/splitting-types.html&#34; target=&#34;_blank&#34;&gt;Amazon Machine Learning Developer Guide&lt;/a&gt;
seems supporting the sequential splitting in our case.
Similar to our prediction problem, it introduces prediction during certain
time range based on previous time range as a representative
example for sequential splitting.&lt;/p&gt;

&lt;h2 id=&#34;2-proportion-of-train-dev-test-sets&#34;&gt;(2) Proportion of train/dev/test sets&lt;/h2&gt;

&lt;p&gt;What proportions of dataset are generally assigned for dev and test sets?
According to the online lecture &amp;ldquo;&lt;a href=&#34;https://www.coursera.org/lecture/deep-neural-network/train-dev-test-sets-cxG1s&#34; target=&#34;_blank&#34;&gt;Train/Dev/ Test sets&lt;/a&gt;&amp;rdquo; by
Andrew Ng, it depends on the size of dataset.
For example, if the total number of data is about 100, 1000, or 10,000,
dividing train/dev/test into 60%/20%/20% is more general.
However, if your dataset including more than about a million records,
assigning 10,000 records for each of dev and test sets would be okay
to obtain a reliable accuracy test result.&lt;/p&gt;

&lt;p&gt;Our dataset has far more than a million records so assigning 10,000 to
100,000 records during the last time period would be okay according to
the lecture. However, if I consider the number of events in the dataset,
because it&amp;rsquo;s a rare event, 10,000 records would only
include 10 to 100 events which makes me not sure whether this is enough
size to measure the accuracy correctly.&lt;/p&gt;

&lt;h2 id=&#34;3-when-most-of-training-data-and-upcoming-target-data-have-different-distribution&#34;&gt;(3) When most of training data and upcoming target data have different distribution&lt;/h2&gt;

&lt;p&gt;The Medium article,
&lt;a href=&#34;https://medium.com/@anilvedala/mismatched-training-and-dev-sets-in-deep-learning-5c84b2b6d207&#34; target=&#34;_blank&#34;&gt;Mismatched training and dev sets in Deep Learning&lt;/a&gt;,
deal with the situation when the distribution of train set would be different
from that of test set or future target data. For instance, to develop a deep learning
algorithm that classifies cat pictures from non-cat pictures in mobile phone,
developer collected a large amount of high-quality of pictures and
comparatively small number of low-quality mobile pictures, which is closer
to the target data.&lt;/p&gt;

&lt;p&gt;In that situation, the article recommends dividing the small mobile picture data
into the three sets of dataset &amp;mdash; train, dev (using in validation process),
and test sets &amp;mdash; with the ratio about 2:1:1. Then, by comparing the accuracies
on four sets of data &amp;mdash; train, train+dev, dev, and test,
we can diagnose the model whether it&amp;rsquo;s overfitted or underfitted.
Additionally, the lecture &amp;ldquo;&lt;a href=&#34;https://www.coursera.org/lecture/deep-neural-network/train-dev-test-sets-cxG1s&#34; target=&#34;_blank&#34;&gt;Train/Dev/Test sets&lt;/a&gt;&amp;rdquo; emphasizes to make sure
that dev set and test set are from a same distribution.&lt;/p&gt;

&lt;p&gt;The dataset of our project doesn&amp;rsquo;t have the different distribution issue,
so this section is more for the future.&lt;/p&gt;

&lt;h1 id=&#34;3-examples-from-previous-researches&#34;&gt;3. Examples from previous researches&lt;/h1&gt;

&lt;h2 id=&#34;1-25-25-50&#34;&gt;(1) 25% - 25% - 50%&lt;/h2&gt;

&lt;p&gt;In a &lt;a href=&#34;https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5176360/&#34; target=&#34;_blank&#34;&gt;recent research&lt;/a&gt;
on the prediction of short-term mortality,
dataset is generated through random sampling of 20,000 patients for
each type of target diseases, from the national samples including 0.8 to
2.7 million people depending on disease. Then they split equally into
train and test sets, that is, 10,000 for each set. The train set is again
splited into train and dev sets for parameter optimization, that is,
5,000 samples for each.&lt;/p&gt;

&lt;p&gt;Similar to our case, the target class of the prediction is a rare cases
(10% or under), they applied both oversampling - creating duplicates of
the minority class - and undersampling - random sampling of non-events
only for train set and validate it with original (non-event biased) dev set.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Style in the Age of Instagram</title>
      <link>https://jaehyukpark.github.io/publication/style_in_the_age_of_instagram_cscw_2016/</link>
      <pubDate>Sat, 27 Feb 2016 21:16:05 -0400</pubDate>
      
      <guid>https://jaehyukpark.github.io/publication/style_in_the_age_of_instagram_cscw_2016/</guid>
      <description></description>
    </item>
    
  </channel>
</rss>
